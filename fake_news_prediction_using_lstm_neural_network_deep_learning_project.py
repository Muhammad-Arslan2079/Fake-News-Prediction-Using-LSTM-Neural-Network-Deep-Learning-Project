# -*- coding: utf-8 -*-
"""Fake News Prediction Using LSTM Neural Network-Deep Learning Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dIEbE7wW6DAJja5OQkgoF6Yg6RYUL_iU

**Importing Libraries**
"""

import numpy as np # for numpy arrays
import pandas as pd # to load dataset and data analysis
import re # regular expression library for fetching and patterns and expressions in data
from nltk.corpus import stopwords  # for comparing and removing less important,common words
from nltk.stem.porter import PorterStemmer
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential # to create sequential models in which layers can be linearly stacked
from tensorflow.keras.layers import Dense,Embedding,LSTM
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import os  # for accessing operating system environment variables to create folders,setting paths
import json # read and write json format file
from zipfile import ZipFile # to unzip compressed downloaded dataset

"""**Data Collection through Kaggle API**"""

!pip install kaggle # installing kaggle to access datasets
kaggle_dictionary= json.load(open('/content/kaggle.json')) #loading kaggle. json format file in kaggle dictionary variable

"""**Setting up Kaggle credentials**"""

kaggle_dictionary.keys() # dictionary has key value pairs
os.environ['KAGGLE_USERNAME']= kaggle_dictionary['username'] # accessing environment variables and saving in dictionary
os.environ['KAGGLE_KEY']= kaggle_dictionary['key']

"""**Downloading kaggle dataset using API Command**"""

#!/bin/bash
!kaggle datasets download jruvika/fake-news-detection # downloading dataset in compressed zip form

"""**Unziping the compressed kaggle dataset**"""

with ZipFile('/content/fake-news-detection.zip') as unzipped: # unzip the compressed folder
   unzipped.extractall()

"""**Dataset Loading and Preprocessing**"""

df=pd.read_csv('/content/data.csv') # creating dataframe and loading dataset
df.head() # first 5 rows

df.tail() # last 5 rows

df.drop(columns='URLs',axis=1,inplace=True) # droping urls

df['Text']= df['Headline']+df['Body'] # merging headline and body

df.drop(columns=['Headline','Body'],axis=1,inplace=True)

df.head()

df.isnull().sum() # checking null values

df=df.dropna() #dropping null values

"""**Downloading stopwords**"""

import nltk # importing nltk library for text preprocessing
nltk.download('stopwords') # importing stop words from library
print(stopwords.words('english')) # printing stopwords in imported from library to compare with data

stemmed= PorterStemmer() # storing stemming function in variable

def cleaned(Text): #creating a function named as cleaned and providing argument as Text
  cleaned_text= re.sub('[^a-z A-Z\s]','',Text) # re function will substitute every thing except upper,lower casealphabets and space
  cleaned_text= cleaned_text.lower() # will lower case all alphabets to maintain same format
  cleaned_text= cleaned_text.split() # splits the text
  cleaned_text= [stemmed.stem(word) for word in cleaned_text if not word in stopwords.words('english')] #applying for loop for stemming the words to their root form and removing stopwords
  cleaned_text= ' '.join(cleaned_text) # again joining splited words into single string
  return cleaned_text

df['Text']= df['Text'].apply(cleaned) # applying function to dataframe Text feature

"""**Separating train and test data**"""

train_data,test_data=train_test_split(df,test_size=0.2,random_state=40)

print(train_data.shape) # checking rows and columns
print(test_data.shape)

"""**Tokenization**"""

tokenizer= Tokenizer(num_words=6000) # most frequent common occuring 6000 words while ignoring all other data text
tokenized= tokenizer.fit_on_texts(train_data['Text']) # training tokenizer
paded_sequence_trained=pad_sequences(tokenizer.texts_to_sequences(train_data['Text']),maxlen=200) # converting text to tokens by tokenization and ensuring same length through padding
X_train=paded_sequence_trained
paded_sequence_test= pad_sequences(tokenizer.texts_to_sequences(test_data['Text']),maxlen=200) # same on test data
X_test= paded_sequence_test

Y_train= train_data['Label'] # creatin target features from splited data
Y_test=test_data['Label']

print(X_train)
print(X_test)

"""**Building LSTM Model**"""

model= Sequential()
model.add(Embedding(input_dim=6000,output_dim=256,input_length=200,input_shape=(200,)))
model.add(LSTM(256,dropout=0.2,recurrent_dropout=0.2))
model.add(Dense(1,activation='sigmoid'))

model.summary()

"""**Model compiling**"""

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

"""**Model Training**"""

model.fit(X_train,Y_train,epochs=5,batch_size=64,validation_split=0.2)

"""**Model Evaluation**"""

loss,accuracy=model.evaluate(X_test,Y_test)
print(f'The accuracy of model is:{accuracy}')
print(f'The loss value is:{loss}')

"""**As we have no information about labels being Fake  and authentic because the dataset is itself encoded already,so I am assuming 0 as authentic and 1 as fake news label, You can change it as per your data set **

**Building a predictive Syestem**
"""

def predict_news(Text):
  tokenized_text= tokenizer.texts_to_sequences(Text)
  paded_sequences=pad_sequences(tokenized_text,maxlen=200)
  prediction= model.predict(paded_sequences)
  news= 'Authentic news' if prediction[0][0] > 0.5 else 'Fake News'
  return news

# sample news
sample_news= 'imran khan won election'
news= predict_news([sample_news])
print(f'{news}')